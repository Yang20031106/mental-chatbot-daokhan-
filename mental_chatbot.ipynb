{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9118b5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in .\\venv\\lib\\site-packages (from -r requirements.txt (line 1)) (1.49.0)\n",
      "Requirement already satisfied: pandas in .\\venv\\lib\\site-packages (from -r requirements.txt (line 2)) (2.3.2)\n",
      "Requirement already satisfied: numpy in .\\venv\\lib\\site-packages (from -r requirements.txt (line 3)) (2.2.6)\n",
      "Requirement already satisfied: scikit-learn in .\\venv\\lib\\site-packages (from -r requirements.txt (line 4)) (1.7.1)\n",
      "Requirement already satisfied: rouge-score in .\\venv\\lib\\site-packages (from -r requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: pyspellchecker in .\\venv\\lib\\site-packages (from -r requirements.txt (line 6)) (0.8.3)\n",
      "Requirement already satisfied: joblib in .\\venv\\lib\\site-packages (from -r requirements.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: matplotlib in .\\venv\\lib\\site-packages (from -r requirements.txt (line 8)) (3.10.5)\n",
      "Requirement already satisfied: packaging<26,>=20 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (0.9.1)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (9.1.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (11.3.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (6.0.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (6.32.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (0.10.2)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (6.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (5.5.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (6.2.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (3.1.45)\n",
      "Requirement already satisfied: pyarrow>=7.0 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (21.0.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in .\\venv\\lib\\site-packages (from streamlit->-r requirements.txt (line 1)) (8.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in .\\venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in .\\venv\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in .\\venv\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.15.3)\n",
      "Requirement already satisfied: absl-py in .\\venv\\lib\\site-packages (from rouge-score->-r requirements.txt (line 5)) (2.3.1)\n",
      "Requirement already satisfied: nltk in .\\venv\\lib\\site-packages (from rouge-score->-r requirements.txt (line 5)) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in .\\venv\\lib\\site-packages (from rouge-score->-r requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 8)) (4.59.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 8)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 8)) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 8)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in .\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 8)) (3.2.3)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in .\\venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: jinja2 in .\\venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in .\\venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (4.25.1)\n",
      "Requirement already satisfied: colorama in .\\venv\\lib\\site-packages (from click<9,>=7.0->streamlit->-r requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in .\\venv\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 1)) (4.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\venv\\lib\\site-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in .\\venv\\lib\\site-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\venv\\lib\\site-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\venv\\lib\\site-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: tqdm in .\\venv\\lib\\site-packages (from nltk->rouge-score->-r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in .\\venv\\lib\\site-packages (from nltk->rouge-score->-r requirements.txt (line 5)) (2025.7.34)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in .\\venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 1)) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\venv\\lib\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in .\\venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in .\\venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (2025.4.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in .\\venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (0.27.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in .\\venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (0.36.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfa24721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    text    intent  emotion\n",
      "0                               hi there  greeting    happy\n",
      "1  can you tell me ways to manage stress       faq  neutral\n",
      "2                     thanks for talking  farewell  neutral\n",
      "3                 i feel so tense lately     other  anxious\n",
      "4                           good morning  greeting    happy\n",
      "intent\n",
      "faq         119\n",
      "other        87\n",
      "greeting     69\n",
      "farewell     59\n",
      "Name: count, dtype: int64\n",
      "emotion\n",
      "neutral      80\n",
      "suicidal     73\n",
      "happy        61\n",
      "depressed    49\n",
      "anxious      43\n",
      "angry        28\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df_intent = pd.read_csv(\"data/intent_emotion.csv\", sep=\"\\t\", header=None)\n",
    "df_intent.columns = [\"text\", \"intent\", \"emotion\"]\n",
    "\n",
    "# Check data\n",
    "print(df_intent.head())\n",
    "print(df_intent['intent'].value_counts())\n",
    "print(df_intent['emotion'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ff24931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\limda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\limda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\limda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\limda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    text              clean_text\n",
      "0                               hi there                      hi\n",
      "1  can you tell me ways to manage stress  tell way manage stress\n",
      "2                     thanks for talking          thanks talking\n",
      "3                 i feel so tense lately       feel tense lately\n",
      "4                           good morning            good morning\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "spell = SpellChecker()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    corrected = []\n",
    "    for word in tokens:\n",
    "        if word in stop_words or word.strip() == \"\":\n",
    "            corrected.append(word)\n",
    "        else:\n",
    "            # pyspellchecker might return None, so we fallback to the original word\n",
    "            c = spell.correction(word)\n",
    "            corrected.append(c if c is not None else word)\n",
    "    \n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in corrected if word not in stop_words]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "\n",
    "df_intent['clean_text'] = df_intent['text'].apply(preprocess_text)\n",
    "print(df_intent[['text', 'clean_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "142a91e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Intent Classification ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         faq       0.83      0.83      0.83        24\n",
      "    farewell       1.00      1.00      1.00        12\n",
      "    greeting       1.00      0.93      0.96        14\n",
      "       other       0.72      0.76      0.74        17\n",
      "\n",
      "    accuracy                           0.87        67\n",
      "   macro avg       0.89      0.88      0.88        67\n",
      "weighted avg       0.87      0.87      0.87        67\n",
      "\n",
      "=== Emotion Classification ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       1.00      0.60      0.75         5\n",
      "     anxious       1.00      0.56      0.71         9\n",
      "   depressed       0.38      0.30      0.33        10\n",
      "       happy       0.62      0.67      0.64        12\n",
      "     neutral       0.42      0.62      0.50        16\n",
      "    suicidal       0.64      0.60      0.62        15\n",
      "\n",
      "    accuracy                           0.57        67\n",
      "   macro avg       0.67      0.56      0.59        67\n",
      "weighted avg       0.62      0.57      0.57        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ---------------------------\n",
    "# 3a: Intent Classifier\n",
    "# ---------------------------\n",
    "X_intent = df_intent['clean_text']\n",
    "y_intent = df_intent['intent']  # 'faq', 'greeting', 'farewell', 'other'\n",
    "\n",
    "# Train/test split\n",
    "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(\n",
    "    X_intent, y_intent, test_size=0.2, random_state=42, stratify=y_intent\n",
    ")\n",
    "\n",
    "# Pipeline: TF-IDF + Logistic Regression\n",
    "intent_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=200))\n",
    "])\n",
    "\n",
    "# Train\n",
    "intent_pipeline.fit(X_train_i, y_train_i)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred_i = intent_pipeline.predict(X_test_i)\n",
    "print(\"=== Intent Classification ===\")\n",
    "print(classification_report(y_test_i, y_pred_i))\n",
    "\n",
    "# ---------------------------\n",
    "# 3b: Emotion Classifier\n",
    "# ---------------------------\n",
    "X_emotion = df_intent['clean_text']\n",
    "y_emotion = df_intent['emotion']  # 'anxious', 'depressed', 'happy', 'angry', 'suicidal'\n",
    "\n",
    "# Train/test split\n",
    "X_train_e, X_test_e, y_train_e, y_test_e = train_test_split(\n",
    "    X_emotion, y_emotion, test_size=0.2, random_state=42, stratify=y_emotion\n",
    ")\n",
    "\n",
    "# Pipeline: TF-IDF + Logistic Regression\n",
    "emotion_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=200))\n",
    "])\n",
    "\n",
    "# Train\n",
    "emotion_pipeline.fit(X_train_e, y_train_e)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred_e = emotion_pipeline.predict(X_test_e)\n",
    "print(\"=== Emotion Classification ===\")\n",
    "print(classification_report(y_test_e, y_pred_e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82343310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load FAQ dataset\n",
    "df_faq = pd.read_csv(\"data/faq_qa.csv\", sep=\"\\t\", header=None)\n",
    "df_faq.columns = [\"question\", \"answer\"]\n",
    "\n",
    "# Preprocess FAQ questions (reuse the same preprocessing function)\n",
    "df_faq['clean_question'] = df_faq['question'].apply(preprocess_text)\n",
    "\n",
    "# Vectorize FAQ questions\n",
    "faq_vectorizer = TfidfVectorizer()\n",
    "faq_vectors = faq_vectorizer.fit_transform(df_faq['clean_question'])\n",
    "\n",
    "# Emotion-based template responses\n",
    "emotion_templates = {\n",
    "    \"anxious\": \"Take a deep breath and calm down. \",\n",
    "    \"depressed\": \"I understand you feel down. \",\n",
    "    \"happy\": \"Glad to hear from you! \",\n",
    "    \"angry\": \"I hear your frustration. \",\n",
    "    \"suicidal\": \"Please reach out to a professional immediately: \"\n",
    "}\n",
    "\n",
    "# Function to get chatbot response\n",
    "def get_response(user_input):\n",
    "    clean_input = preprocess_text(user_input)\n",
    "\n",
    "    # Predict intent\n",
    "    proba = intent_pipeline.predict_proba([clean_input])[0]\n",
    "    best_idx = proba.argmax()\n",
    "    intent_conf = proba[best_idx]\n",
    "    intent = intent_pipeline.classes_[best_idx]\n",
    "    \n",
    "    if intent_conf < 0.4:\n",
    "        intent = \"fallback\"\n",
    "\n",
    "    # Predict emotion\n",
    "    proba_e = emotion_pipeline.predict_proba([clean_input])[0]\n",
    "    best_idx_e = proba_e.argmax()\n",
    "    emotion_conf = proba_e[best_idx_e]\n",
    "    emotion = emotion_pipeline.classes_[best_idx_e]\n",
    "    \n",
    "    if emotion_conf < 0.3:\n",
    "        emotion = \"neutral\"\n",
    "\n",
    "    # Generate response\n",
    "    if intent == \"faq\":\n",
    "        user_vector = faq_vectorizer.transform([clean_input])\n",
    "        similarities = cosine_similarity(user_vector, faq_vectorizer.transform(df_faq['clean_question']))\n",
    "        best_idx = similarities.argmax()\n",
    "        answer = df_faq.iloc[best_idx]['answer']\n",
    "        response = f\"{emotion_templates.get(emotion,'')}{answer}\"\n",
    "    elif intent == \"greeting\":\n",
    "        response = f\"{emotion_templates.get(emotion,'')}Hello! How can I help you today?\"\n",
    "    elif intent == \"farewell\":\n",
    "        response = f\"{emotion_templates.get(emotion,'')}Goodbye! Take care.\"\n",
    "    elif intent == \"fallback\":\n",
    "        response = \"I'm not sure I understand. Can you rephrase your question?\"\n",
    "    else:\n",
    "        response = f\"{emotion_templates.get(emotion,'')}I'm here to listen. Tell me more.\"\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6675e337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models and data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create model folder if it doesn't exist\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "# --- Save intent pipeline ---\n",
    "with open(\"model/intent_pipeline.pkl\", \"wb\") as f:\n",
    "    pickle.dump(intent_pipeline, f)\n",
    "\n",
    "# --- Save emotion pipeline ---\n",
    "with open(\"model/emotion_pipeline.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emotion_pipeline, f)\n",
    "\n",
    "# --- Save FAQ vectorizer ---\n",
    "with open(\"model/faq_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(faq_vectorizer, f)\n",
    "\n",
    "# --- Save preprocessed FAQ dataset ---\n",
    "df_faq.to_pickle(\"model/faq_dataset.pkl\")\n",
    "\n",
    "print(\"All models and data saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
